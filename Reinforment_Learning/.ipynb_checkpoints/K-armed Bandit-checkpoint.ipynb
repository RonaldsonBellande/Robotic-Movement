{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class K_armed_Bandit_Problem(object):\n",
    "    def __init__(self, number_bandit_problems = 500, k = 7, epsilon = 0.1, c = 1):\n",
    "        self.number_bandit_problems = number_bandit_problems\n",
    "        self.k = k\n",
    "        self.epsilon = epsilon\n",
    "        self.c = c\n",
    "     \n",
    "    # Return Q*(a)\n",
    "    def k_armed_bandit(self):\n",
    "        return np.random.normal(0,1,(self.number_bandit_problems,k))\n",
    "    \n",
    "    \n",
    "    def play_k_armed_bandit(self, q_values, problem_number = 0, time_step = 1000, strategy = 'greedy'):\n",
    "        \n",
    "        q = np.zeros(self.k)\n",
    "        rewards = np.zeroes(time_step)\n",
    "        act = np.zeros(time_step)\n",
    "        actions = np.zeros(k)\n",
    "        \n",
    "        if stratagy == 'greedy':\n",
    "            for i in range(time_step):\n",
    "                action = np.argmax(q)\n",
    "                act[i] = action\n",
    "                rewards[i] = np.random.normal(q_values[problem_number][action],1)\n",
    "                actions[action] += 1\n",
    "                q[action]=q[action]+1/actions[action]*(rewards[i]-q[action])\n",
    "                \n",
    "            optimum_action = np.argmax(q_values[problem_number])\n",
    "            opt_act=act==optimum_action\n",
    "            return act,rewards,opt_act\n",
    "        \n",
    "        elif strategy == 'epsilon_greedy':\n",
    "            for i in range(time_steps):\n",
    "                if np.random.uniform(0,1) < self.epsilon:\n",
    "                    action=np.random.randint(0,self.k)\n",
    "                    act[i]=action\n",
    "                    rewards[i]=np.random.normal(q_values[problem_number][action],1)\n",
    "                    actions[action]+=1\n",
    "                    q[action]=q[action]+1/actions[action]*(rewards[i]-q[action])\n",
    "                else:\n",
    "                    action=np.argmax(q)\n",
    "                    act[i]=action\n",
    "                    rewards[i]=np.random.normal(q_values[problem_number][action],1)\n",
    "                    actions[action]+=1\n",
    "                    q[action]=q[action]+1/actions[action]*(rewards[i]-q[action])\n",
    "        \n",
    "            optimum_action = np.argmax(q_values[problem_number])\n",
    "            opt_act=act==optimum_action\n",
    "            return act,rewards,opt_act\n",
    "        \n",
    "        elif strategy=='ucb':\n",
    "            for i in range(time_steps):\n",
    "                temp=np.zeros(self.k)\n",
    "                for j in range(self.k):\n",
    "                    temp[j]=q[j]+c*np.sqrt(np.log(i+1)/actions[j])\n",
    "                action=np.argmax(temp)\n",
    "                act[i]=action\n",
    "            \n",
    "                rewards[i]=np.random.normal(q_values[problem_number][action],1)\n",
    "                actions[action]+=1\n",
    "                q[action]=q[action]+1/actions[action]*(rewards[i]-q[action])\n",
    "            \n",
    "            optimum_action = np.argmax(q_values[problem_number])\n",
    "            opt_act=act==optimum_action\n",
    "            return act,rewards,opt_act\n",
    "        \n",
    "        \n",
    "    def simulate_all(self, number_of_arms, number_of_runs, number_of_time_step):\n",
    "        \n",
    "        greedy_rewards=np.zeros((number_of_runs,number_of_time_step))\n",
    "        greedy_actions=np.zeros((number_of_runs,number_of_time_step))\n",
    "        greedy_opt=np.zeros((number_of_runs,number_of_time_step))\n",
    "        epsilon01_rewards = np.zeros((number_of_runs,number_of_time_step))\n",
    "        epsilon1_rewards=np.zeros((number_of_runs,number_of_time_step))\n",
    "        epsilon01_actions=np.zeros((number_of_runs,number_of_time_step))\n",
    "        epsilon1_actions=np.zeros((number_of_runs,number_of_time_step))\n",
    "        epsilon01_opt=np.zeros((number_of_runs,number_of_time_step))\n",
    "        epsilon1_opt=np.zeros((number_of_runs,number_of_time_step))\n",
    "        ucb1_rewards=np.zeros((number_of_runs,number_of_time_step))\n",
    "        ucb2_rewards=np.zeros((number_of_runs,number_of_time_step))\n",
    "        ucb1_actions=np.zeros((number_of_runs,number_of_time_step))\n",
    "        ucb2_actions=np.zeros((number_of_runs,number_of_time_step))\n",
    "        ucb1_opt=np.zeros((number_of_runs,number_of_time_step))\n",
    "        ucb2_opt=np.zeros((number_of_runs,number_of_time_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
